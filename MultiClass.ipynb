{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using Plots, Interact, LinearAlgebra\n",
    "using Random: randperm\n",
    "using Statistics: mean\n",
    "\n",
    "theme(\n",
    "    :wong;\n",
    "    markerstrokecolor=\"white\", \n",
    "    markerstrokewidth=0,\n",
    "    alpha=0.6,\n",
    "    label=\"\"\n",
    ")\n",
    "\n",
    "function heatmap_digit(x::AbstractMatrix; kwargs...)\n",
    "    return heatmap(\n",
    "        x;\n",
    "        transpose=true,\n",
    "        yflip=true,\n",
    "        showaxis=:false,\n",
    "        grid=:false,\n",
    "        color=:grays,\n",
    "        aspect_ratio=1.0,\n",
    "        kwargs...\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "grad_loss_1layer_1output"
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dW, db, loss = grad_loss_1layer_1output(f_a, df_a, X, y, W, b)\n",
    "\n",
    "Evaluate the loss function and its gradient at `X`.\n",
    "\n",
    "**Inputs**\n",
    "* `f_a`, `df_a`: activation function and its derivative, respectively\n",
    "* `X`: *d* x *N* input matrix (*N* samples with dimension *d*)\n",
    "* `y`: *1* x *N* output vector (function values corresponding to the *N* samples in `X`)\n",
    "* `W`: *n* x *d* weight matrix, where n is the hidden layer dimension\n",
    "* `b`: length-*n* bias vector\n",
    "\n",
    "**Outputs**\n",
    "* `dW`: vector of gradients with respect to weights\n",
    "* `db`: gradient with respect to bias\n",
    "* `loss`: loss function value\n",
    "\"\"\"\n",
    "function grad_loss_1layer_1output(\n",
    "        f_a::Function,\n",
    "        df_a::Function,\n",
    "        X::AbstractMatrix,\n",
    "        y::AbstractMatrix,\n",
    "        W::AbstractMatrix,\n",
    "        b::AbstractVector\n",
    "    )\n",
    "\n",
    "    n, d = size(W)\n",
    "    N = size(X, 2)\n",
    "\n",
    "    dW = zeros(n, d)\n",
    "    db = zeros(n)\n",
    "    loss = 0.0\n",
    "\n",
    "    for k in 1:N\n",
    "        error = y[k] - sum(f_a.(W * X[:, k] + b))\n",
    "        for p in 1:n\n",
    "            for q in 1:d\n",
    "                dW[p, q] -= 2 / N * error * df_a(W[p, :]' * X[:, k] + b[p]) * X[q, k]\n",
    "            end\n",
    "            db[p] -= 2 / N * error * df_a(W[p, :]' * X[:, k] + b[p])\n",
    "        end\n",
    "\n",
    "        loss += 1/N * error^2\n",
    "    end\n",
    "    return dW, db, loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "learn2classify_sgd_1layer"
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random: randperm, seed!\n",
    "\n",
    "\"\"\"\n",
    "    W, b, loss = learn2classify_sgd_1layer(f_a, df_a, grad_loss, X, y, W0, b0[,\n",
    "        mu=1e-3, iters=500, batch_size=10, seed=0])\n",
    "\n",
    "Perform stochastic gradient descent and return optimized weights `W`, bias `b`,\n",
    "    and vector of loss function values `loss`.\n",
    "\n",
    "**Inputs**\n",
    "* `f_a`, `df_a`: activation function and its derivative, respectively\n",
    "* `grad_loss`: gradient of the loss function\n",
    "* `X`: d x N matrix representing N samples having dimension d\n",
    "* `y`: 1 x N row matrix containing scalar function values for the N input samples\n",
    "* `W0`: n x d initial weight matrix, where n is the hidden layer dimension\n",
    "* `b0`: length-n initial bias vector\n",
    "* `mu`: gradient descent step size\n",
    "* `iters`: number of gradient descent iterations to perform\n",
    "* `batch_size`: number of samples to use for each iteration\n",
    "* `seed`: specify a random seed for batch selection\n",
    "\n",
    "**Outputs**\n",
    "* `W`: optimized matrix of weights\n",
    "* `b`: optimized bias vector\n",
    "* `loss`: vector of loss function values for all iterations\n",
    "\"\"\"\n",
    "function learn2classify_sgd_1layer(\n",
    "        f_a::Function,\n",
    "        df_a::Function,\n",
    "        grad_loss::Function,\n",
    "        X::AbstractMatrix,\n",
    "        y::AbstractMatrix,\n",
    "        W0::AbstractMatrix,\n",
    "        b0::AbstractVector,\n",
    "        mu::Number=1e-3,\n",
    "        iters::Integer=500,\n",
    "        batch_size::Integer=10,\n",
    "        seed::Integer=0\n",
    "    )\n",
    "    \n",
    "    (seed != 0) && seed!(seed) # use seed if provided\n",
    "    \n",
    "    n, d = size(W0) # number of hidden neurons, inputs\n",
    "    N = size(X, 2) # number of training samples\n",
    " \n",
    "    W = W0\n",
    "    b = b0\n",
    "    loss = zeros(iters)\n",
    "    for i in 1:iters\n",
    "        batch_idx = randperm(N)\n",
    "        batch_idx = batch_idx[1:min(batch_size, N)]\n",
    "        \n",
    "        dW, db, loss_i = grad_loss(\n",
    "            f_a, df_a, X[:, batch_idx], y[:, batch_idx], W, b)\n",
    "        \n",
    "        W -= mu * dW\n",
    "        b -= mu * db\n",
    "\n",
    "        loss[i] = loss_i\n",
    "    end\n",
    "    return W, b, loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "learn2classify_asgd_1layer"
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random: randperm, seed!\n",
    "\n",
    "\"\"\"\n",
    "    W, b, loss = learn2classify_asgd_1layer(f_a, df_a, grad_loss, X, y, W0, b0[,\n",
    "        mu=1e-3, iters=500, batch_size=10, seed=0])\n",
    "\n",
    "Perform accelerated stochastic gradient descent and return optimized weights\n",
    "    `W`, bias `b`, and vector of loss function values `loss`.\n",
    "\n",
    "**Inputs**\n",
    "* `f_a`, `df_a`: activation function and its derivative, respectively\n",
    "* `grad_loss`: gradient of the loss function\n",
    "* `X`: d x N matrix representing N samples having dimension d\n",
    "* `y`: 1 x N row matrix containing scalar function values for the N input samples\n",
    "* `W0`: n x d initial weight matrix, where n is the hidden layer dimension\n",
    "* `b0`: length-n initial bias vector\n",
    "* `mu`: gradient descent step size\n",
    "* `iters`: number of gradient descent iterations to perform\n",
    "* `batch_size`: number of samples to use for each iteration\n",
    "* `seed`: specify a random seed for batch selection\n",
    "\n",
    "**Outputs**\n",
    "* `W`: optimized matrix of weights\n",
    "* `b`: optimized bias vector\n",
    "* `loss`: vector of loss function values for all iterations\n",
    "\"\"\"\n",
    "function learn2classify_asgd_1layer(\n",
    "        f_a::Function, \n",
    "        df_a::Function, \n",
    "        grad_loss::Function,\n",
    "        X::AbstractMatrix, \n",
    "        y::AbstractMatrix, \n",
    "        W0::AbstractMatrix, \n",
    "        b0::AbstractVector,\n",
    "        mu::Number=1e-3, \n",
    "        iters::Integer=500, \n",
    "        batch_size::Integer=10,\n",
    "        seed::Integer=0\n",
    "    )\n",
    "    \n",
    "    (seed != 0) && seed!(seed) # use seed if provided\n",
    "\n",
    "    d = size(W0, 2) #number of inputs\n",
    "    n = size(W0, 1) # number of neurons\n",
    "    N = size(X, 2) # number of training samples\n",
    " \n",
    "    W = W0\n",
    "    b = b0\n",
    "    \n",
    "    loss = zeros(iters)\n",
    "\n",
    "    lambda_k = 0\n",
    "    q_k = W\n",
    "    p_k = b\n",
    "    for i in 1:iters\n",
    "        batch_idx = randperm(N)\n",
    "        batch_idx = batch_idx[1:min(batch_size, N)]\n",
    "        \n",
    "        dW, db, loss_i = grad_loss(f_a, df_a, X[:, batch_idx], y[:, batch_idx], W, b)\n",
    "        \n",
    "        q_kp1 = W - mu * dW\n",
    "        p_kp1 = b - mu * db\n",
    "\n",
    "        lambda_kp1 = (1 + sqrt(1 + 4 * lambda_k^2)) / 2\n",
    "        gamma_k = (1 - lambda_k) / lambda_kp1\n",
    "\n",
    "        W = (1 - gamma_k) * q_kp1 + gamma_k * q_k\n",
    "        b = (1 - gamma_k) * p_kp1 + gamma_k * p_k\n",
    "\n",
    "        q_k = q_kp1\n",
    "        p_k = p_kp1\n",
    "        lambda_k = lambda_kp1\n",
    "\n",
    "        loss[i] = loss_i\n",
    "    end\n",
    "    return W, b, loss\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "name": "julia-1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}